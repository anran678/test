""" Train for model.
    Config:
        train_dataset:
          dataset: $spec; wrapper: $spec; batch_size:
        val_dataset:
          dataset: $spec; wrapper: $spec; batch_size:
        (data_norm):
            inp: {sub: []; div: []}
            gt: {sub: []; div: []}
        (eval_type):
        (eval_bsize):

        model: $spec
        optimizer: $spec
        epoch_max:
        (multi_step_lr):
            milestones: []; gamma: 0.5
        (resume): *.pth

        (epoch_val): ; (epoch_save):
"""

import argparse
import os
import numpy as np
import random
import yaml
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from torch.utils.data import DataLoader
import torch.nn.utils  as nn_utils
from torch.optim.lr_scheduler import MultiStepLR

from torchprofile import profile_macs

import datasets
import models
import utils
from test import eval_psnr
from utils import make_coord
from utils import to_pixel_samples


def calculate_flops_and_params(model, input_size):
    # 创建输入张量
    input_tensor = torch.randn(*input_size).cuda()

    # 计算参数量
    total_params1 = sum(p.numel() for p in model.parameters())
    
    scale = 4
    h_hr = input_size[-2]*scale
    w_hr = input_size[-1]*scale
    
    hr = torch.randn(*[1,3,h_hr,w_hr]).cuda()
    
    coord, _ = to_pixel_samples(hr.contiguous())
    coord = coord.unsqueeze(0).cuda()
       
    '''coord = make_coord((h_hr, w_hr), flatten=False).cuda()
    coord = coord.unsqueeze(0)'''
    cell = torch.tensor([1 / h_hr, 1 / w_hr], dtype=torch.float32).repeat(h_hr*w_hr, 1).unsqueeze(0).cuda()
    
    scale = torch.ones_like(coord).cuda()
    scale[:, 0] *= 1 / h_hr
    scale[:, 1] *= 1 / w_hr
    # scale = scale.unsqueeze(0)

    # 计算 FLOPS
    macs = profile_macs(model, args=(input_tensor, coord, scale))
    flops1 = 2 * macs  # 每个 MAC 约等于 2 FLOPS
    
    print(f"FLOPS: {flops1}")
    print(f"Parameters: {total_params1}")
    
    total_params2 = sum(p.numel() for p in model.encoder.parameters())
    # 计算 FLOPS
    macs = profile_macs(model.encoder, args=(input_tensor))
    flops2 = 2 * macs  # 每个 MAC 约等于 2 FLOPS
    
    print(f"FLOPS: {flops2}")
    print(f"Parameters: {total_params2}")
    
    flops = flops1-flops2
    total_params = total_params1-total_params2
    
    return flops, total_params


def prepare_training():
    if config.get('pre_train') is not None:
        print('loading pre_train model... ', config['pre_train'])
        model = models.make(config['model']).cuda()
        model_dict = model.state_dict()

        #load pre_train parameters
        sv_file = torch.load(config['pre_train'])
        pretrained_dict = sv_file['model']['sd']
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict }
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)
        
        #load pre_train parameters
        optimizer = utils.make_optimizer(
            model.parameters(), config['optimizer'])
        epoch_start = 1
        if config.get('multi_step_lr') is None:
            lr_scheduler = None
        else:
            lr_scheduler = MultiStepLR(optimizer, **config['multi_step_lr'])
            
    elif config.get('resume') is not None:
       
        sv_file = torch.load(config['resume'])
        model = models.make(sv_file['model'], load_sd=True).cuda()
        optimizer = utils.make_optimizer(
            model.parameters(), sv_file['optimizer'], load_sd=True)
        epoch_start = sv_file['epoch'] + 1
        state = sv_file['state']
        torch.set_rng_state(state)
        print(f'Resuming from epoch {epoch_start}...')
        if config.get('multi_step_lr') is None:
            lr_scheduler = None
        else:
            lr_scheduler = MultiStepLR(optimizer, **config['multi_step_lr'])
        
        lr_scheduler.last_epoch = epoch_start - 1

    else:
        print('prepare_training from start')
        model = models.make(config['model']).cuda()
        optimizer = utils.make_optimizer(
            model.parameters(), config['optimizer'])
        epoch_start = 1
        if config.get('multi_step_lr') is None:
            lr_scheduler = None
        else:
            lr_scheduler = MultiStepLR(optimizer, **config['multi_step_lr'])
        for _ in range(epoch_start - 1):
            lr_scheduler.step()

    log('model: #params={}'.format(utils.compute_num_params(model, text=True)))
    log('model: #struct={}'.format(model))
    return model, optimizer, epoch_start, lr_scheduler

        

def main(config_, save_path):
    
    global config, log, writer
    config = config_
    log, writer = utils.set_save_path(save_path)
    with open(os.path.join(save_path, 'config.yaml'), 'w') as f:
        yaml.dump(config, f, sort_keys=False)
    
    model, optimizer, epoch_start, lr_scheduler = prepare_training()
    
    input_size = (1, 3, 48, 48)

    # 计算 FLOPS 和参数量
    flops, params = calculate_flops_and_params(model, input_size)
    print(f"FLOPS: {flops}")
    print(f"Parameters: {params}")

    
        


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', default='configs/train/train_itnsr.yaml')
    parser.add_argument('--name', default=None)
    parser.add_argument('--tag', default=None)
    parser.add_argument('--gpu', default='0')
    args = parser.parse_args()

    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    
    def setup_seed(seed):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed) # sets the seed for cpu
        torch.cuda.manual_seed(seed) # Sets the seed for the current GPU.
        torch.cuda.manual_seed_all(seed) #  Sets the seed for the all GPU.
        # torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark=True
    setup_seed(2021)
    
    with open(args.config, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
        print('config loaded.')

    save_name = args.name
    if save_name is None:
        save_name = '_' + args.config.split('/')[-1][:-len('.yaml')]
    if args.tag is not None:
        save_name += '_' + args.tag
    save_path = os.path.join('./save', save_name)

    main(config, save_path)
